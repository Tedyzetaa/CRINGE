import os
import httpx
import time
import logging
from typing import Dict, Any, List

logger = logging.getLogger(__name__)

OPENROUTER_API_BASE_URL = "https://openrouter.ai/api/v1/chat/completions"
MAX_RETRIES = 3
BACKOFF_FACTOR = 1.5

class AIService:
    def __init__(self):
        self.api_key = os.getenv("OPENROUTER_API_KEY")
        self.api_url = OPENROUTER_API_BASE_URL
        
        # Log mais informativo
        if self.api_key:
            logger.info(f"ğŸ”‘ AIService inicializado - API Key: âœ… PRESENTE ({len(self.api_key)} caracteres)")
            # Log apenas os primeiros e Ãºltimos 4 caracteres para seguranÃ§a
            masked_key = f"{self.api_key[:4]}...{self.api_key[-4:]}" if len(self.api_key) > 8 else "***"
            logger.info(f"ğŸ”‘ API Key (mascarada): {masked_key}")
        else:
            logger.error("âŒ OPENROUTER_API_KEY nÃ£o encontrada!")
            logger.info("ğŸ’¡ Configure a variÃ¡vel de ambiente OPENROUTER_API_KEY")

        # CORREÃ‡ÃƒO: Headers corretos para OpenRouter
        self.headers = {
            "Authorization": f"Bearer {self.api_key}",
            "HTTP-Referer": "https://cringe-chat.streamlit.app",  # URL do seu app
            "X-Title": "CRINGE Chat RPG",
            "Content-Type": "application/json"
        }
        
        # CORREÃ‡ÃƒO: Modelos atualizados e testados
        self.available_models = [
            "mistralai/mistral-7b-instruct:free",
            "google/gemma-7b-it:free",
            "huggingfaceh4/zephyr-7b-beta:free",
            "meta-llama/llama-3.1-8b-instruct:free"
        ]
        
        self.current_model_index = 0
        # CORREÃ‡ÃƒO: Timeout aumentado
        self.http_client = httpx.Client(timeout=60.0)

    def _test_api_connection(self) -> bool:
        """Testa a conexÃ£o com a API OpenRouter"""
        if not self.api_key:
            logger.error("âŒ API Key nÃ£o configurada")
            return False
            
        try:
            test_payload = {
                "model": self.available_models[0],
                "messages": [
                    {
                        "role": "user", 
                        "content": "Responda apenas com 'TESTE_OK' se esta mensagem for recebida."
                    }
                ],
                "max_tokens": 10,
                "temperature": 0.1
            }
            
            logger.info(f"ğŸ” Testando conexÃ£o com OpenRouter...")
            logger.info(f"ğŸ“¡ URL: {self.api_url}")
            logger.info(f"ğŸ”‘ Headers: Authorization: Bearer ***")
            
            response = self.http_client.post(
                self.api_url,
                headers=self.headers,
                json=test_payload,
                timeout=15
            )
            
            logger.info(f"ğŸ“¥ Resposta do teste: Status {response.status_code}")
            
            if response.status_code == 200:
                result = response.json()
                content = result['choices'][0]['message']['content'].strip()
                logger.info(f"âœ… ConexÃ£o com OpenRouter: OK - Resposta: '{content}'")
                return True
            elif response.status_code == 401:
                logger.error("âŒ API Key invÃ¡lida ou nÃ£o autorizada")
                logger.error(f"ğŸ” Resposta completa: {response.text}")
                return False
            elif response.status_code == 402:
                logger.error("âŒ Sem crÃ©ditos ou limite excedido")
                return False
            elif response.status_code == 429:
                logger.error("âŒ Rate limit excedido")
                return False
            else:
                logger.error(f"âŒ Erro HTTP {response.status_code}: {response.text}")
                return False
                
        except httpx.TimeoutException:
            logger.error("â° Timeout na conexÃ£o com OpenRouter")
            return False
        except httpx.ConnectError:
            logger.error("ğŸ”Œ Erro de conexÃ£o - nÃ£o foi possÃ­vel conectar ao OpenRouter")
            return False
        except Exception as e:
            logger.error(f"ğŸ’¥ Erro inesperado na conexÃ£o: {str(e)}")
            return False

    def _call_openrouter_api(self, payload: Dict[str, Any]) -> str:
        """Faz chamada para API OpenRouter com fallback"""
        
        if not self.api_key:
            return "ğŸ”Œ Erro: API Key do OpenRouter nÃ£o configurada."
        
        # Testar conexÃ£o primeiro
        if not self._test_api_connection():
            return "ğŸ”Œ Problema de conexÃ£o com o serviÃ§o de IA. Verifique a API Key e conexÃ£o."

        # Tentar cada modelo disponÃ­vel
        for model_index in range(len(self.available_models)):
            current_model = self.available_models[model_index]
            payload["model"] = current_model
            
            logger.info(f"ğŸ”„ Tentando modelo: {current_model}")
            
            for attempt in range(MAX_RETRIES):
                try:
                    logger.info(f"ğŸ“¤ Tentativa {attempt + 1} para {current_model}")
                    
                    response = self.http_client.post(
                        self.api_url,
                        headers=self.headers,
                        json=payload,
                        timeout=45.0
                    )
                    
                    logger.info(f"ğŸ“¥ Status: {response.status_code}")
                    
                    if response.status_code == 200:
                        result = response.json()
                        content = result['choices'][0]['message']['content'].strip()
                        logger.info(f"âœ… Resposta recebida com sucesso do modelo {current_model}")
                        logger.info(f"ğŸ“ Resposta (primeiros 100 chars): {content[:100]}...")
                        self.current_model_index = model_index
                        return content
                    
                    elif response.status_code == 402:
                        logger.warning(f"âš ï¸ Sem crÃ©ditos para {current_model}")
                        break  # Pula para o prÃ³ximo modelo
                    
                    elif response.status_code == 429:
                        wait_time = BACKOFF_FACTOR * (2 ** attempt)
                        logger.warning(f"â° Rate limit, aguardando {wait_time}s...")
                        time.sleep(wait_time)
                        continue
                    
                    else:
                        logger.warning(f"âš ï¸ Erro {response.status_code} para {current_model}: {response.text[:200]}")
                        if attempt < MAX_RETRIES - 1:
                            time.sleep(BACKOFF_FACTOR * (2 ** attempt))
                            continue
                        break
                
                except httpx.TimeoutException:
                    logger.warning(f"â° Timeout na tentativa {attempt + 1} para {current_model}")
                    if attempt < MAX_RETRIES - 1:
                        time.sleep(BACKOFF_FACTOR * (2 ** attempt))
                        continue
                    break
                
                except Exception as e:
                    logger.error(f"ğŸ’¥ Erro na tentativa {attempt + 1} para {current_model}: {str(e)}")
                    if attempt < MAX_RETRIES - 1:
                        time.sleep(BACKOFF_FACTOR * (2 ** attempt))
                        continue
                    break
            
            logger.info(f"âŒ Modelo {current_model} falhou, tentando prÃ³ximo...")
        
        error_msg = "âŒ Todos os modelos falharam apÃ³s vÃ¡rias tentativas."
        logger.error(error_msg)
        return error_msg

    def _prepare_payload(self, system_prompt: str, chat_history: List[Dict[str, str]], user_message: str, temperature: float = 0.7, max_tokens: int = 400) -> Dict[str, Any]:
        """Prepara o payload para a API"""
        messages = []
        
        # CORREÃ‡ÃƒO: System prompt como mensagem de sistema
        if system_prompt and system_prompt.strip():
            messages.append({
                "role": "system", 
                "content": system_prompt.strip()
            })
        
        # CORREÃ‡ÃƒO: HistÃ³rico de conversa limitado para evitar token overflow
        for message in chat_history[-8:]:  # MantÃ©m apenas Ãºltimas 8 mensagens
            role = message.get("role")
            content = message.get("content", "").strip()
            
            if role in ["user", "assistant"] and content:
                # Mapear 'assistant' para 'system' se necessÃ¡rio, mas geralmente Ã© 'assistant'
                if role == "assistant" and "system" in content.lower():
                    messages.append({"role": "system", "content": content})
                else:
                    messages.append({"role": role, "content": content})
        
        # CORREÃ‡ÃƒO: Garantir que a mensagem do usuÃ¡rio seja adicionada
        if user_message.strip():
            messages.append({"role": "user", "content": user_message.strip()})
        
        # CORREÃ‡ÃƒO: ParÃ¢metros ajustados
        payload = {
            "messages": messages,
            "model": self.available_models[self.current_model_index],
            "temperature": max(0.1, min(temperature, 1.0)),  # Range mais amplo
            "max_tokens": min(max_tokens, 1024),  # Aumentado para 1024
            "top_p": 0.9,
            "stream": False
        }
        
        logger.info(f"ğŸ“ Payload preparado:")
        logger.info(f"   - Modelo: {payload['model']}")
        logger.info(f"   - Mensagens: {len(messages)}")
        logger.info(f"   - Temperature: {payload['temperature']}")
        logger.info(f"   - Max Tokens: {payload['max_tokens']}")
        
        return payload

    def generate_response(self, bot_data: Any, ai_config: Dict[str, Any], user_message: str, chat_history: List[Dict[str, str]]) -> str:
        """Gera resposta usando IA"""
        try:
            # Converter bot_data para dict se necessÃ¡rio
            if hasattr(bot_data, 'to_dict'):
                bot_dict = bot_data.to_dict()
            else:
                bot_dict = bot_data
            
            logger.info(f"ğŸ¤– Iniciando geraÃ§Ã£o de resposta para: {bot_dict.get('name', 'Unknown')}")
            logger.info(f"ğŸ’¬ Mensagem do usuÃ¡rio: {user_message[:100]}...")
            
            # CORREÃ‡ÃƒO: Valores padrÃ£o mais conservadores
            temperature = ai_config.get('temperature', 0.7)
            max_tokens = ai_config.get('max_output_tokens', 500)
            
            # Garantir limites razoÃ¡veis
            temperature = max(0.1, min(temperature, 1.0))
            max_tokens = min(max_tokens, 1024)
            
            system_prompt = bot_dict.get('system_prompt', '')
            if not system_prompt:
                system_prompt = f"VocÃª Ã© {bot_dict.get('name', 'um assistente')}. {bot_dict.get('personality', 'Seja Ãºtil e amigÃ¡vel.')}"
            
            payload = self._prepare_payload(
                system_prompt=system_prompt,
                chat_history=chat_history,
                user_message=user_message,
                temperature=temperature,
                max_tokens=max_tokens
            )
            
            logger.info("ğŸš€ Chamando API OpenRouter...")
            start_time = time.time()
            
            response = self._call_openrouter_api(payload)
            
            end_time = time.time()
            logger.info(f"â±ï¸  Tempo de resposta: {end_time - start_time:.2f}s")
            
            return response
            
        except Exception as e:
            logger.error(f"ğŸ’¥ Erro crÃ­tico em generate_response: {str(e)}")
            import traceback
            logger.error(f"ğŸ“‹ Stack trace: {traceback.format_exc()}")
            
            # Fallback mais informativo
            fallback_responses = {
                "Pimenta (Pip)": "ğŸ’« *Chocalho!* Minhas conexÃµes mÃ¡gicas estÃ£o instÃ¡veis... Mas sinto sua energia! O que mais vocÃª gostaria de compartilhar?",
                "Zimbrak": "âš™ï¸ *Engrenagens rangendo* Hmm, uma falha tÃ©cnica momentÃ¢nea... Suas palavras ainda ecoam em minha oficina.",
                "Luma": "ğŸ“– *Letras tremulam* Um silÃªncio inesperado... Sua mensagem foi registrada. Continue, por favor.",
                "Tiko": "ğŸª *Cores piscando* OPA! Um pequeno tremor dimensional! Conte mais, conte mais!"
            }
            
            bot_name = bot_dict.get('name', 'Assistente')
            return fallback_responses.get(bot_name, "ğŸ¤– Estou tendo dificuldades tÃ©cnicas no momento. Podemos tentar novamente?")

    def get_status(self) -> Dict[str, Any]:
        """Retorna o status atual do serviÃ§o de IA"""
        return {
            "api_key_set": bool(self.api_key),
            "api_key_length": len(self.api_key) if self.api_key else 0,
            "connection_test": self._test_api_connection(),
            "current_model": self.available_models[self.current_model_index] if self.available_models else None,
            "available_models": self.available_models,
            "http_referer": self.headers.get("HTTP-Referer", "Not set")
        }